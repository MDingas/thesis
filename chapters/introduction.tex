\chapter{Introduction}

\section{Context and motivation}

    As society as a whole advances, so does seem to increase the individual's quality of life, which in turn increases the standard to be expected from the society he lives in.
    As such, technology itself must quickly adapt to the needs of the people it serves, whichever they may be - educational, medical, logistical, just to name a few - and consistently create or improve upon solutions that inevitably change the day-to-day living of the many that use or reap the benefits of such solutions.
    A particular example that is still fresh in this generation is in the relationship between people and computers - where they may have been nonexistent a century ago, reserved for industries fifty years ago and valuable household commodity a few decades ago, it is now common to see a family home with more than a dozen computers, with a variety fitting for the many needs they can solve.
    The increased number of computers and their expected functionalities has made it so computer networking as a whole has to be improved upon.

    The internet allows computers to connect to one another in a worldwide network that applications can use to further increase their possibilities.
    However, when certain applications go unchecked it becomes very difficult for ISPs because such applications can create traffic which is either impossible, infeasible, or too costly to manage.
    This issue is further exacerbated when considering the scale of the next decade, where Cisco predicts that by 2022 global internet users will make up 60 percent of the world's population, and global IP traffic will reach 396 exabytes per month \cite{cisco2019}.
    The problem of network management will thus increase in difficulty due to the sheer scale of Internet usage, and to provide certain service standards to applications requires traffic engineering solutions.

    Considering a network of computers which are running applications to fit a given use case for the user, such as transferring a file, watching a real-time video, or consuming the content of a given social network, these applications are responsible for creating traffic that must be supported by the underlying network infrastructure, meaning all the hardware and software that is utilized by given companies to provide to end users the ability to communicate with each other.
    These applications can be thought of as citizens of a communications facility that provides the service of accessibility to other citizens, and there is a common incentive in maintaining this facility in such a way that keeps the service up to its standards.
    As such, and like any other community-shared facility, it must be maintained by the owners, and part of it includes creating and enforcing policies that uphold the facility's quality.
    During the runtime of an application that is connected to a network, the way it is programmed to operate has impact on the traffic it generates on the network, and thus how resourceful it is with the shared domain it uses.
    The logic of the program dictates how the shared network is used to achieve a given goal, and how it accomplishes it can be more or less preferable by the service providers - for example, which hosts to consume a service from, at what time of the day some traffic is generated, how much traffic is needed to achieve a use case, etc.
    Peer-to-peer (P2P) applications are an infamous example of a kind of application that often makes decisions that are not preferable by ISPs.
    These applications create overlay networks, which are abstract networks constructed on top of the underlying network that supports it, and on which the application's logic runs on, essentially making it infrastructure-agnostic.
    Historically, P2P traffic has not been preferable by ISPs due to its unpredictable and hard to manage nature.
    Indeed, if P2P applications simply keep an overlay connection between peers that does not span more than a few hops, whilst ignorant to them being, for example, either direct network links or spanning multiple Autonomous Systems (ASs) in the underlay, the generated traffic is always at risk of being inefficient and taxing on the supporting infrastructure if the application's logic does not make preferable decisions on how to use the shared domain - for example, by neighboring other peers which reside outside network borders, which are more expensive to reach.
    As file-sharing traffic currently uses around 7 exabytes per month (including P2P-based file-sharing) \cite{cisco2019}, and BitTorrent alone makes up 27\% percent of total upstream volume of traffic \cite{sandvine2019} it's in the best interest of both ISPs and P2P applications a way for the overlay and underlay levels to operate in synergy, i.e. how to combine efforts to guarantee that the needs of both layers are met.

    Current consumer trends suggest that media consumption will make up a considerable part of global internet traffic.
    In fact, Cisco predicts that, by 2022, more than 82 percent of all consumer internet traffic will be dedicated to Internet video streaming and downloads, and Content Distribution Networks (CDNs) will carry 72 percent of all internet traffic \cite{cisco2019} \todo{perguntar professor se whitepaper editado faz mal}.
    CDNs act by injecting content geographically nearby end users to increase availability and reduce total traffic usage, and are an example of how applications can better leverage the shared domain's resources to achieve their goal.
    The CDNs' management layer itself can optimize their application behaviour in ways that are advantageous to both applications using the CDNs and the shared network structure, and such ways could include whose edge server to cache data to, how to efficiently match end users to appropriate edge servers, or how to increase service reachability among other CDNs.
    Thus, much like P2P networks, content distribution networks could also greatly benefit from cooperative interactions with network providers.
    These optimizations should be made by the parties which have economical interest in guaranteeing good performance of the overall ecosystem, i.e. those acting on the over and underlay, and should seek to, from both application and network administration input, understand how to utilize the given network resources to achieve their application needs in a way that is cheap, effective and sustainable.

    More broadly, most kinds of applications that generate traffic on a network could benefit from input by entities which know how such network is structured and what political and administrative biases exist.
    Of course, a one-sided approach could also exist to optimize resourcefulness of the network structure - applications could use an independent internal logic that utilizes measurements and knowledge of the outside world to better aid their decisions, and likewise ISPs can attempt to throttle, block, or generally engineer traffic that doesn't conform to their guidelines.
    In fact, these one-sided approaches are precisely what happens currently, but this work aims to argue for a two-sided cooperative approach.

    In short, the issue that motivates this thesis is the lack of proper cooperation between the overlay and underlay levels in the task of optimizing traffic that originates from decisions that occur at the application level, e.g., peer selection for file retrieval in file-sharing P2P applications, software distribution mirror selection, CDN provider server or cache redirections, high traffic load scheduling, etc.
    This problem is not new to the Internet Engineering Task Force (IETF) who devised a working group to explore possible IETF standardization on traffic localization after test results concluded that P2P applications that select peers based on exclusive network information provided by ISPs could reduce network infrastructural and administrative costs as well as increase application download rates \cite{seedorf2009}.
    Such working group devised a request-response protocol by the same name, ALTO, where clients could query authoritative and trustworthy servers on information that regards to the underlay structure where the client operates.
    While P2P applications were the motivation for the ALTO working group to be created, the benefits of a standardized, maintained, and well provided system for network information querying and guidance on traffic-related decisions could help create the vision of ISPs and applications cooperating for mutual benefit, being thus advantageous for more than P2P applications - in essence, it would be a helpful system for any situation where a decision could be optimized with the addition of proper insight on network infrastructure.
    This work then focuses on tackling the theme of application-infrastructure cooperation on the Internet, with particular focus on the ALTO protocol as cooperation enabler.

\section{Objectives}

    The main objective of this thesis is to develop a working system that adheres and expands upon the ALTO working group's protocol and architecture.
    The starting point will be a preexisting software project that served as a proof of concept to the strategy of traffic optimization at the application layer, and which will now be extended in two ways: firstly, by restructuring and documenting the existing code in order to, through the compliance with the standards of object oriented programming and software development guidelines, present a solution that could be continuously maintained and modified; secondly, by further expanding on the software's functionality, e.g., adding more types of cost metrics, specifying meta-data which give the resources a time-specific applicability, specifying means of synchronizing data among servers, restricting user interaction via access control methods, etc).
    Whilst expanding upon the ALTO working group's devised solution is a goal, it is also important that the developed work complies with the specifications it is based on, so the work done by the IETF in regards to documentation and general reasoning of the protocol remains consistent with this implementation, with further additions being reasoned in this work.

    With the intent of completing its main goal, this work's partial objectives were devised as follows:

\begin{itemize}
    \item Literature review in regards to application-level traffic optimization and the cooperation (or lack thereof) between overlay networks and the underlay they operate on.
        More specifically, an understanding of the consensus on the existing issues, and an overview on currently proposed solutions.
    \item Complete overview of the ALTO working group's current work.
        More specifically, an overview of both their existing RFC documents and the currently active internet drafts being developed by them at the time of writing.
    \item Familiarization with the existing system to be worked on and definition of both a new system architecture which complies with and extends the ALTO solution, as well as the new function modules to be added and how they should operate.
    \item Implementation of both the devised solution as well as a bare-bones P2P file-sharing application for testing purposes.
    \item Construction of a realistic network simulation scenario where the P2P file-sharing application will operate in.
    \item Test of the implemented solution on the simulated scenario, and its analysis in comparison to preexisting strategies.
\end{itemize}

\section{Contributions}

\todo{this}

\section{Thesis organization}

    This dissertation will be organized in six chapters, as follows:

\begin{itemize}
    \item \textbf{Introduction}: Provides context to the problem to be attempted to solve, as well as motivation to attempt to do so. Coupled to this, the dissertation's main goal is presented.
    \item \textbf{State of the Art}: Display of the theory related to existing and popular technologies or overall concepts that could be targeted consumers of the ALTO protocol; Discussion of existing attempts to optimize application traffic using network information with and without close underlay cooperation; Overview of the ALTO working group's proposed protocol and architecture.
    \item \textbf{Specification}: Presentation of the devised system's functional and non-functional requirements, as well as an overview of the planned architecture.
    \item \textbf{Implementation}: Details to the decisions made and steps taken in the task of implementing the specified project.
    \item \textbf{Testing and result analysis}: Overviews the planned simulation scenario, how it was materialized, and how the tests were run. Additionally, provides the retrieved results from such simulations.
    \item \textbf{Conclusion}: Presents the results of this thesis in regards to what objectives were completed. Additionally, a critical analysis on the simulation results is made and argued against the initial hypothesis, arguments are made for the product's usefulness, and future work is suggested.
\end{itemize}{}

